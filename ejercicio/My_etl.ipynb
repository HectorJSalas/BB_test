{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716ca680-5ba0-4c6b-be81-23da17b3bad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "libreria requerida para la el proceso ETL \n",
    "\"\"\"\n",
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdbb10a-5fd6-46d9-aa7b-d9bf405ca58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\"\"\"Generacion de variables y Folders en caso de queno exitan\n",
    "un vez generado el folder sera necesario cargar el archivo CSV \n",
    "a nuestra folder RAW\n",
    "\"\"\"\n",
    "RAW_FOLDER='./raw/'\n",
    "PROCESSED_FOLDER='./processed/'\n",
    "RAW_FILE='data_prueba_tecnica.csv'\n",
    "PROCESSED_FILE='data_processed.csv'\n",
    "PROCESSES_VALIDATION_FILE='data_validation.csv'\n",
    "SEP=','\n",
    "ENCODING='utf-8'\n",
    "\n",
    "os.makedirs(RAW_FOLDER, exist_ok=True)\n",
    "os.makedirs(PROCESSED_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1667b9c4-6582-41bb-893a-7c2e701a69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_data(path_file, file_name, sep, econding):\n",
    "    \"\"\"Lectura de la archivo de capa raw \n",
    "    Args:\n",
    "        path_file (string): Directorio del archivo\n",
    "        file_name (string): Nombre del archivo\n",
    "        sep (string) : Separador de columnas de nuestro archivo\n",
    "        encoding (string) : Codoficacion del archivo\n",
    "    Returns:\n",
    "        dataframe: Dataframe con los valores de raw\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_file+file_name, sep=sep, encoding=econding)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bece252-bec1-47c3-8774-90c2b4d166c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Limpieza y tranformacion del data frame generdo en la capa raw de lectura,\n",
    "    esta funcion elimina duplicados, nulos. transfroma los campos creat_at y paid_at a tipo fecha\n",
    "    ademas de generar como reaultado 2 data frames.\n",
    "    Args:\n",
    "        df(dataframe): datfreme de lectira de capa raw \n",
    "    Returns:\n",
    "        df_clean: Datafreme tranformado y limpio\n",
    "        df_inc:  Dtafrem con valores inconsistentes\n",
    "    \"\"\"\n",
    "    \n",
    "    # print(df.columns)\n",
    "    # print(df.dtypes)\n",
    "    # nulos_por_columna = df.isnull().sum()\n",
    "    # print(nulos_por_columna)\n",
    "    # print(df.info)\n",
    "    df_clean = df.drop_duplicates()\n",
    "    df_clean['created_at'] = pd.to_datetime(\n",
    "        df_clean['created_at'], errors='coerce')\n",
    "    df_clean['paid_at'] = pd.to_datetime(df_clean['paid_at'], errors='coerce')\n",
    "    df_inc = df[df_clean[['created_at', 'id',\n",
    "                          'name', 'company_id']].isnull().any(axis=1)]\n",
    "    df_clean = df_clean.dropna(\n",
    "        subset=['created_at', 'id', 'name', 'company_id'])\n",
    "\n",
    "    # print(df_clean)\n",
    "\n",
    "    return df_clean, df_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c56728f3-20d0-4e0d-977b-2541cbe4cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, file_path, file_name, sep, encoding):\n",
    "    \"\"\"Guardado de un archivo en capa processed a partir de un dataframe \n",
    "\n",
    "    Args:\n",
    "        df (dataframe): Data a guardar \n",
    "        path_file (string): Directorio del archivo\n",
    "        file_name (string): Nombre del archivo\n",
    "        sep (string) : Separador de columnas de nuestro archivo\n",
    "        encoding (string) : Codoficacion del archivo\n",
    "    Returns:\n",
    "        dataframe: Dataframe con los valores de raw\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path+file_name, sep=sep, encoding=encoding, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccbe300-9946-45d9-b759-d9efe3f55c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ventas_dia(df):\n",
    "    \"\"\"\"\n",
    "    Generacion de agraciones sobre los campos name ,\n",
    "    creare_at renombrado date y amount renombrado total_amount \n",
    "    Args:\n",
    "        df(dataframe): Datfreme transformdo y limpio \n",
    "    Returns:\n",
    "        df (dataframe): Datafreme con agregacion a nivel dia\n",
    "    \"\"\"\n",
    "    df = df.groupby(['name', df['created_at'].dt.date])[\n",
    "        'amount'].sum().reset_index()\n",
    "    df.rename(columns={'created_at': 'date',\n",
    "                    'amount': 'total_amount'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7aba2ae5-8bf7-4c56-9c43-416cf06d56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patron_pago(df):\n",
    "    \"\"\"\"\n",
    "    Generacion de agraciones sobre los campos name ,\n",
    "    creare_at renombrado date y amount renombrado total_amount \n",
    "    Args:\n",
    "        df(dataframe): Datfreme transformdo y limpio\n",
    "    Returns:\n",
    "        df_clean: Datafreme tranformado y limpio\n",
    "    \"\"\"\n",
    "    df_pagado = df[df['status'] == 'paid'].copy()\n",
    "\n",
    "    # 2. Calcular el tiempo de pago (solo para las transacciones filtradas)\n",
    "    df_pagado['tiempo_pago'] = (df_pagado['paid_at'] - df_pagado['created_at']).dt.days\n",
    "\n",
    "    # 3. Calcular la desviación estándar del tiempo de pago por empresa\n",
    "    # regularidad = df_pagado.groupby('company_id')['tiempo_pago'].std().reset_index()\n",
    "    regularidad = df_pagado.groupby('company_id')['tiempo_pago'].std().reset_index()\n",
    "    regularidad.rename(columns={'tiempo_pago': 'desviacion_estandar'}, inplace=True)\n",
    "\n",
    "    # 4. Clasificación según la desviación estándar\n",
    "    def clasificar_regularidad(std):\n",
    "        if std < 10:\n",
    "            return \"Alta Regularidad\"\n",
    "        elif 10 <= std <= 30:\n",
    "            return \"Moderada Regularidad\"\n",
    "        else:\n",
    "            return \"Irregular\"\n",
    "\n",
    "    regularidad['clasificacion'] = regularidad['desviacion_estandar'].apply(clasificar_regularidad)\n",
    "\n",
    "    # Resultados\n",
    "    # print(\"DataFrame filtrado (status = 'pagado'):\")\n",
    "    # print(df_pagado)\n",
    "\n",
    "    print(\"\\nRegularidad de las empresas:\")\n",
    "    print(regularidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d09b2e8-0208-4139-9043-cc4d6cd16d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regularidad de las empresas:\n",
      "                                 company_id  desviacion_estandar  \\\n",
      "0                                   *******                  NaN   \n",
      "1  8f642dc67fccf861548dfe1c761ce22f795e91f0             0.678911   \n",
      "2  cbf1c8b09cd5b549416d49d220a40cbd317f952e             0.248195   \n",
      "\n",
      "      clasificacion  \n",
      "0         Irregular  \n",
      "1  Alta Regularidad  \n",
      "2  Alta Regularidad  \n"
     ]
    }
   ],
   "source": [
    "df_raw = read_raw_data(RAW_FOLDER,\n",
    "                       RAW_FILE, SEP, ENCODING)\n",
    "\n",
    "df_clean, df_inc = clean_data(df_raw)\n",
    "df_clean_agg=ventas_dia(df_clean)\n",
    "df_patron_pago=patron_pago(df_clean)\n",
    "save_data(df_clean_agg, PROCESSED_FOLDER,\n",
    "          PROCESSED_FILE, ',', 'utf-8')\n",
    "save_data(df_inc, PROCESSED_FOLDER,\n",
    "          'data_validation.csv', ',', 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f1055-d8ed-4739-b90d-1925f987ffa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d57c0-489e-4cdb-b12b-cb61efb3d388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
